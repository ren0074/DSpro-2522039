{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bc359bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visiting: https://www.musashino-u.ac.jp/ (Remaining to visit: 0, Visited: 1)\n",
      "Visiting: https://www.musashino-u.ac.jp/research/interview/ (Remaining to visit: 98, Visited: 2)\n",
      "Visiting: https://www.musashino-u.ac.jp/research/laboratory/institute_of_Law.html (Remaining to visit: 196, Visited: 3)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 105\u001b[0m\n\u001b[1;32m    102\u001b[0m     url_title_dict[current_url] \u001b[38;5;241m=\u001b[39m page_title\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# ページ内のリンクを探索\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     new_links \u001b[38;5;241m=\u001b[39m \u001b[43mfind_links\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     to_visit\u001b[38;5;241m.\u001b[39mupdate(new_links \u001b[38;5;241m-\u001b[39m visited)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# 結果を表示\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 57\u001b[0m, in \u001b[0;36mfind_links\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     55\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url, headers\u001b[38;5;241m=\u001b[39mheaders, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     56\u001b[0m response\u001b[38;5;241m.\u001b[39mraise_for_status() \u001b[38;5;66;03m# HTTPエラー（4xx, 5xx）があれば例外を発生させる\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCRAWL_DELAY_SECONDS\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# 各リクエスト後に待機\u001b[39;00m\n\u001b[1;32m     58\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m anchor \u001b[38;5;129;01min\u001b[39;00m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m, href\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import time\n",
    "\n",
    "# 起点 URL\n",
    "base_url = \"https://www.musashino-u.ac.jp/\"  # 武蔵野大学Webサイトのトップページ\n",
    "\n",
    "# 辿ったURLとタイトルを保存するための辞書\n",
    "url_title_dict = {}\n",
    "\n",
    "# リンクを巡回するためのセット\n",
    "to_visit = set([base_url]) \n",
    "visited = set() \n",
    "\n",
    "# ロボット除けとユーザーエージェントの設定\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',\n",
    "    'Accept-Encoding': 'gzip, deflate, br'\n",
    "}\n",
    "\n",
    "CRAWL_DELAY_SECONDS = 0.8\n",
    "\n",
    "def normalize_url(url):\n",
    "    \"\"\"\n",
    "    URLを正規化する関数。\n",
    "    重複を避けるため、クエリパラメータとフラグメントを除去します。\n",
    "    \"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    return urljoin(f\"{parsed.scheme}://{parsed.netloc}\", parsed.path)\n",
    "\n",
    "def get_title(url):\n",
    "    \"\"\"ページの<title>を取得する関数\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status() # HTTPエラー（4xx, 5xx）があれば例外を発生させる\n",
    "        time.sleep(CRAWL_DELAY_SECONDS) # 各リクエスト後に待機\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        # タイトルがNoneでないことと、文字列であることを確認し、空白を除去\n",
    "        title = soup.title.string.strip() if soup.title and soup.title.string else \"No Title\"\n",
    "        # 連続する空白等を正規化 (オプション)\n",
    "        return ' '.join(title.split())\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error accessing {url}: {e}\")\n",
    "        return f\"Fetching Error: {type(e).__name__}\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {url}: {e}\")\n",
    "        return f\"Processing Error: {type(e).__name__}\"\n",
    "\n",
    "def find_links(url):\n",
    "    \"\"\"指定したURLに存在する、同一ドメイン内のリンクをすべて取得\"\"\"\n",
    "    links = set()\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status() # HTTPエラー（4xx, 5xx）があれば例外を発生させる\n",
    "        time.sleep(CRAWL_DELAY_SECONDS) # 各リクエスト後に待機\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        for anchor in soup.find_all(\"a\", href=True):\n",
    "            href = anchor[\"href\"]\n",
    "\n",
    "            # JavaScriptリンクやアンカーリンク（ページ内ジャンプ）を除外\n",
    "            if href.startswith(\"javascript:\") or href.startswith(\"#\"):\n",
    "                continue\n",
    "\n",
    "            # 絶対URLに変換\n",
    "            absolute_url = urljoin(url, href)\n",
    "\n",
    "            # 正規化\n",
    "            normalized_absolute_url = normalize_url(absolute_url)\n",
    "\n",
    "            # ドメインチェック\n",
    "            base_hostname = urlparse(base_url).hostname\n",
    "            link_hostname = urlparse(normalized_absolute_url).hostname\n",
    "\n",
    "            # ホスト名が base_hostname またはそのサブドメインであるかチェック\n",
    "            if link_hostname and (link_hostname == base_hostname or link_hostname.endswith('.' + base_hostname)):\n",
    "                 links.add(normalized_absolute_url)\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching links from {url}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing links from {url}: {e}\")\n",
    "    return links\n",
    "\n",
    "# メインループ: リンクを辿る (BFS)\n",
    "while to_visit:\n",
    "    # 巡回するURLを to_visit から一つ取り出す\n",
    "    current_url = to_visit.pop()\n",
    "\n",
    "    # 既に訪問済みならスキップ (これがないと無限ループや重複クロールが発生)\n",
    "    if current_url in visited:\n",
    "        continue\n",
    "\n",
    "    # 訪問済みリストに加える\n",
    "    visited.add(current_url)\n",
    "    print(f\"Visiting: {current_url} (Remaining to visit: {len(to_visit)}, Visited: {len(visited)})\")\n",
    "\n",
    "    # URLの<title>を辞書に格納\n",
    "    page_title = get_title(current_url)\n",
    "    url_title_dict[current_url] = page_title\n",
    "\n",
    "    # ページ内のリンクを探索\n",
    "    new_links = find_links(current_url)\n",
    "\n",
    "    to_visit.update(new_links - visited)\n",
    "\n",
    "# 結果を表示\n",
    "print(\"\\n--- Crawling Completed ---\")\n",
    "print(\"Total URLs found:\", len(url_title_dict))\n",
    "for url, title in url_title_dict.items():\n",
    "    print(f\"URL: {url}\\nTitle: {title}\\n---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
