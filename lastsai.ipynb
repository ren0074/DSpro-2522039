{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bedc80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import time\n",
    "\n",
    "# 起点 URL\n",
    "base_url = \"https://www.musashino-u.ac.jp/\"  # 武蔵野大学Webサイトのトップページ\n",
    "\n",
    "# 辿ったURLとタイトルを保存するための辞書\n",
    "url_title_dict = {}\n",
    "\n",
    "# リンクを巡回するためのセット\n",
    "to_visit = set([base_url]) \n",
    "visited = set() \n",
    "\n",
    "# ロボット除けとユーザーエージェントの設定\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',\n",
    "    'Accept-Encoding': 'gzip, deflate, br'\n",
    "}\n",
    "\n",
    "CRAWL_DELAY_SECONDS = 0.8\n",
    "\n",
    "def normalize_url(url):\n",
    "    \"\"\"\n",
    "    URLを正規化する関数。\n",
    "    重複を避けるため、クエリパラメータとフラグメントを除去します。\n",
    "    \"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    return urljoin(f\"{parsed.scheme}://{parsed.netloc}\", parsed.path)\n",
    "\n",
    "def get_title(url):\n",
    "    \"\"\"ページの<title>を取得する関数\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status() # HTTPエラー（4xx, 5xx）があれば例外を発生させる\n",
    "        time.sleep(CRAWL_DELAY_SECONDS) # 各リクエスト後に待機\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        # タイトルがNoneでないことと、文字列であることを確認し、空白を除去\n",
    "        title = soup.title.string.strip() if soup.title and soup.title.string else \"No Title\"\n",
    "        # 連続する空白等を正規化 (オプション)\n",
    "        return ' '.join(title.split())\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error accessing {url}: {e}\")\n",
    "        return f\"Fetching Error: {type(e).__name__}\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {url}: {e}\")\n",
    "        return f\"Processing Error: {type(e).__name__}\"\n",
    "\n",
    "def find_links(url):\n",
    "    \"\"\"指定したURLに存在する、同一ドメイン内のリンクをすべて取得\"\"\"\n",
    "    links = set()\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status() # HTTPエラー（4xx, 5xx）があれば例外を発生させる\n",
    "        time.sleep(CRAWL_DELAY_SECONDS) # 各リクエスト後に待機\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        for anchor in soup.find_all(\"a\", href=True):\n",
    "            href = anchor[\"href\"]\n",
    "\n",
    "            # JavaScriptリンクやアンカーリンク（ページ内ジャンプ）を除外\n",
    "            if href.startswith(\"javascript:\") or href.startswith(\"#\"):\n",
    "                continue\n",
    "\n",
    "            # 絶対URLに変換\n",
    "            absolute_url = urljoin(url, href)\n",
    "\n",
    "            # 正規化\n",
    "            normalized_absolute_url = normalize_url(absolute_url)\n",
    "\n",
    "            # ドメインチェック\n",
    "            base_hostname = urlparse(base_url).hostname\n",
    "            link_hostname = urlparse(normalized_absolute_url).hostname\n",
    "\n",
    "            # ホスト名が base_hostname またはそのサブドメインであるかチェック\n",
    "            if link_hostname and (link_hostname == base_hostname or link_hostname.endswith('.' + base_hostname)):\n",
    "                 links.add(normalized_absolute_url)\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching links from {url}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing links from {url}: {e}\")\n",
    "    return links\n",
    "\n",
    "# メインループ: リンクを辿る (BFS)\n",
    "while to_visit:\n",
    "    # 巡回するURLを to_visit から一つ取り出す\n",
    "    current_url = to_visit.pop()\n",
    "\n",
    "    # 既に訪問済みならスキップ (これがないと無限ループや重複クロールが発生)\n",
    "    if current_url in visited:\n",
    "        continue\n",
    "\n",
    "    # 訪問済みリストに加える\n",
    "    visited.add(current_url)\n",
    "    print(f\"Visiting: {current_url} (Remaining to visit: {len(to_visit)}, Visited: {len(visited)})\")\n",
    "\n",
    "    # URLの<title>を辞書に格納\n",
    "    page_title = get_title(current_url)\n",
    "    url_title_dict[current_url] = page_title\n",
    "\n",
    "    # ページ内のリンクを探索\n",
    "    new_links = find_links(current_url)\n",
    "\n",
    "    to_visit.update(new_links - visited)\n",
    "\n",
    "# 結果を表示\n",
    "print(\"\\n--- Crawling Completed ---\")\n",
    "print(\"Total URLs found:\", len(url_title_dict))\n",
    "for url, title in url_title_dict.items():\n",
    "    print(f\"URL: {url}\\nTitle: {title}\\n---\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
